{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class for pulling and downloading arxiv papers.\n",
    "\n",
    "Authors:\n",
    "Garrett Williams\n",
    "\"\"\"\n",
    "import arxiv\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import re\n",
    "import inspect\n",
    "import time\n",
    "\n",
    "\n",
    "# Initialize logging.\n",
    "logging_location = os.path.join(os.getcwd(), 'logging')\n",
    "if not (os.path.isdir(logging_location) and os.path.exists(logging_location)):\n",
    "    os.mkdir(logging_location)\n",
    "log_file = os.path.join(os.getcwd(), 'logging', 'logging.log')\n",
    "\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO,\n",
    "                    format=' % (asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "class Arxiv():\n",
    "    \"\"\"\n",
    "    Class for pulling and downloading Arxiv data according to some logic.\n",
    "\n",
    "    Uses the arxiv python wrapper \"arxiv\" found at:\n",
    "        https://github.com/lukasschwab/arxiv.py\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    number_of_repeats: int, optional\n",
    "        Number of times to make call to API. Due to time outs and other issues a single call \n",
    "        may fail to return all desired results.\n",
    "\n",
    "    sort_by: string, optional\n",
    "        Logic for sorting results\n",
    "\n",
    "    max_results: int, optional\n",
    "        Maximum number of results to return from an arxiv query request.\n",
    "\n",
    "    max_chunk_results: int, optional\n",
    "        Maximum number of results to return from a single request to the arxiv API.\n",
    "\n",
    "    iterative: bool, optional\n",
    "        Whether to return results or iterator over results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, number_of_repeats=5, sort_by=\"submittedDate\", max_results=1000, max_chunk_results=10, iterative=True):\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "\n",
    "        self.have_queried_papers = False\n",
    "        self.have_filtered_papers = True\n",
    "\n",
    "    def _search_query(self, categories_filter=True, authors_filter=True, key_phrases_filter=False):\n",
    "        \"\"\"\n",
    "        Function for generating search query for arxiv. The actual categories, authors and key phrases \n",
    "        to filter by are stored in external text files. \n",
    "\n",
    "        Currently the search is generated by taking all authors, key phrases and categories and combining\n",
    "        into a single string by repeated logical ORs.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        categories_filter: Bool, optional\n",
    "            Whether to filter by categories.\n",
    "\n",
    "        author_filter: Bool, optional\n",
    "            Whether to filter by authors.\n",
    "\n",
    "        key_phrases_filter: Bool, optional\n",
    "            Whether to filter using key words.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        search_query: string\n",
    "            Arxiv API compatible search query string.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info('Generated search query.')\n",
    "\n",
    "        categories = \"\"\n",
    "        key_phrases = \"\"\n",
    "        authors = \"\"\n",
    "\n",
    "        if categories_filter:\n",
    "            categories_path = os.getcwd()\n",
    "            categories_path = os.path.join(categories_path, 'categories.txt')\n",
    "\n",
    "            with open(categories_path, 'r') as f:\n",
    "                categories = f.read().splitlines()\n",
    "            categories = '(cat:' + ' OR cat:'.join(categories) + ')'\n",
    "\n",
    "        if authors_filter:\n",
    "            authors_path = os.getcwd()\n",
    "            authors_path = os.path.join(authors_path, 'authors.txt')\n",
    "\n",
    "            with open(authors_path, 'r') as f:\n",
    "                authors = f.read().splitlines()\n",
    "            authors = '(au:' + ' OR au:'.join(authors) + ')'\n",
    "\n",
    "        if key_phrases_filter:\n",
    "            key_phrases_path = os.getcwd()\n",
    "            key_phrases_path = os.path.join(\n",
    "                key_phrases_path, 'key_phrases.txt')\n",
    "\n",
    "            with open(key_phrases_path, 'r') as f:\n",
    "                key_phrases = f.read().splitlines()\n",
    "            key_phrases = '(au:' + ' OR au:'.join(key_phrases) + ')'\n",
    "\n",
    "        self.search_query_string = authors + \" OR \" + categories + \" OR \" + key_phrases\n",
    "\n",
    "    def _generate_papers(self, search_query=None):\n",
    "        \"\"\"\n",
    "        Pulls information on arxiv papers including authors, summary, link, and more. Based off \n",
    "        a wrapper of the Arxiv api found here:\n",
    "        https://github.com/lukasschwab/arxiv.py\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        search_query: string\n",
    "            Arxiv API search query.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        all_papers: list\n",
    "            All papers found by query. \n",
    "        \"\"\"\n",
    "\n",
    "        logging.info('Generating unfiltered list of papers.')\n",
    "\n",
    "        query = self._search_query()\n",
    "\n",
    "        all_papers = []\n",
    "        for _ in range(self.number_of_repeats):\n",
    "            result = arxiv.query(search_query=query,\n",
    "                                 sort_by=self.sort_by,\n",
    "                                 max_results=self.max_results,\n",
    "                                 max_chunk_results=self.max_chunk_results,\n",
    "                                 iterative=self.iterative)\n",
    "            for paper in result():\n",
    "                all_papers.append(paper)\n",
    "            time.sleep(3)\n",
    "\n",
    "        self.papers = list(set(all_papers))\n",
    "\n",
    "        self.have_queried_papers = True\n",
    "\n",
    "    def _filter_papers_time(self, date='Today'):\n",
    "        \"\"\"\n",
    "        Takes a list of arxiv papers and filters the results by date.\n",
    "\n",
    "        Parameter:\n",
    "        ----------\n",
    "        date: str, optional\n",
    "            Date to filter by. If 'All' does no time filtering. If not today or 'All' must be in the form '%Y-%m-%d %H:%M:%S'.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        filtered_results:\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info('Filtering papers by date.')\n",
    "\n",
    "        if not self.have_queried_papers:\n",
    "            self._generate_papers()\n",
    "            if date == 'Today':\n",
    "                today = datetime.today().replace(minute=0, second=0, hour=0,\n",
    "                                                 microsecond=0) - timedelta(days=2)\n",
    "                filtered = []\n",
    "                for paper in self.papers:\n",
    "                    paper_published = datetime.strptime(\n",
    "                        ' '.join(paper['published'].split('T'))[:-1], '%Y-%m-%d %H:%M:%S')\n",
    "                    if paper_published >= today:\n",
    "                        filtered.append(paper)\n",
    "                self.filtered_papers = filtered\n",
    "            elif date == 'All':\n",
    "                self.filtered_papers = self.papers\n",
    "            else:\n",
    "                filter_date = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "                filtered = []\n",
    "                for paper in self.papers:\n",
    "                    paper_published = datetime.strptime(\n",
    "                        ' '.join(paper['published'].split('T'))[:-1], '%Y-%m-%d %H:%M:%S')\n",
    "                    if paper_published >= today:\n",
    "                        filtered.append(paper)\n",
    "                self.filtered_papers = filtered\n",
    "        elif self.have_queried_papers:\n",
    "            if date == 'Today':\n",
    "                today = datetime.today().replace(minute=0, second=0, hour=0,\n",
    "                                                 microsecond=0) - timedelta(days=2)\n",
    "                filtered = []\n",
    "                for paper in self.papers:\n",
    "                    paper_published = datetime.strptime(\n",
    "                        ' '.join(paper['published'].split('T'))[:-1], '%Y-%m-%d %H:%M:%S')\n",
    "                    if paper_published >= today:\n",
    "                        filtered.append(paper)\n",
    "                self.filtered_papers = filtered\n",
    "            elif date == 'All':\n",
    "                self.filtered_papers = self.papers\n",
    "            else:\n",
    "                filter_date = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "                filtered = []\n",
    "                for paper in self.papers:\n",
    "                    paper_published = datetime.strptime(\n",
    "                        ' '.join(paper['published'].split('T'))[:-1], '%Y-%m-%d %H:%M:%S')\n",
    "                    if paper_published >= today:\n",
    "                        filtered.append(paper)\n",
    "                self.filtered_papers = filtered\n",
    "\n",
    "        self.have_filtered_papers = True\n",
    "\n",
    "    def arxiv_papers(self, papers_desired=5, store_papers=False):\n",
    "        \"\"\"\n",
    "        Takes results of _filter_papers_time and filters down to N papers.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        papers_desired: int, optional\n",
    "            Number of papers to keep.\n",
    "\n",
    "        store_papers: Bool, optional\n",
    "            If true, download papers.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info('Applying user defined filters.')\n",
    "\n",
    "        self._filter_papers_time()\n",
    "\n",
    "        if self.have_filtered_papers:\n",
    "            self.fav_papers = self.filtered_papers[:papers_desired]\n",
    "\n",
    "        if store_papers:\n",
    "            self._download_papers()\n",
    "\n",
    "    def _custom_slugify(self, obj):\n",
    "        return re.sub(r'([^\\s\\w]|_)+', '', ' '.join(obj['title'].split()))\n",
    "\n",
    "    def _download_papers(self):\n",
    "        \"\"\"\n",
    "        Method for downloading retrieved papers.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info('Downloading papers locally.')\n",
    "\n",
    "        paper_dump = os.path.join(os.path.normpath(\n",
    "            os.getcwd() + os.sep + os.pardir), 'arxiv_papers')\n",
    "\n",
    "        if not (os.path.isdir(paper_dump) and os.path.exists(paper_dump)):\n",
    "            os.mkdir(paper_dump)\n",
    "        for paper in self.fav_papers:\n",
    "            arxiv.download(paper, dirpath=paper_dump,\n",
    "                           slugify=self._custom_slugify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\logging\\__init__.py\", line 1034, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\logging\\__init__.py\", line 880, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\logging\\__init__.py\", line 622, in format\n",
      "    s = self.formatMessage(record)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\logging\\__init__.py\", line 591, in formatMessage\n",
      "    return self._style.format(record)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\logging\\__init__.py\", line 433, in format\n",
      "    return self._fmt % record.__dict__\n",
      "ValueError: unsupported format character '(' (0x28) at index 3\n",
      "Call stack:\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\asyncio\\base_events.py\", line 528, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1764, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3191, in run_ast_nodes\n",
      "    if (yield from self.run_code(code, result)):\n",
      "  File \"C:\\Users\\Garrett\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-64e7d2d8d5e8>\", line 2, in <module>\n",
      "    example._search_query()\n",
      "  File \"<ipython-input-2-af3cccfc725d>\", line 87, in _search_query\n",
      "    logging.info('Generated search query.')\n",
      "Message: 'Generated search query.'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "example = Arxiv()\n",
    "example._search_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.favorite_papers(papers_desired=15, store_papers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'logging.handlers' from 'C:\\\\Users\\\\Garrett\\\\Anaconda3\\\\lib\\\\logging\\\\handlers.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.handlers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
